# Tabajo 1

## CalibraciÃ³n de la camara

Para la calibraciÃ³n de la cÃ¡mara se siguiÃ³ la guia de GeeksforGeeks (s.f.), con el cual se obtuvieron los siguientes resultados.

La funciÃ³n cv2.drawChessboardCorners() desempeÃ±a un papel crucial al visualizar y verificar el Ã©xito de la detecciÃ³n de esquinas en la imagen, asegurando la calidad de los puntos de referencia. Se empleÃ³ la siguiente lÃ­nea de cÃ³digo para la visualizaciÃ³n:

```python
# Dibujar y mostrar las esquinas en la imagen
image = cv2.drawChessboardCorners(image, 
                                  CHECKERBOARD, 
                                  corners2, ret)
```
En la Figura 1 se muestran cuatro de las imÃ¡genes utilizadas con las esquinas detectadas. Esta detecciÃ³n es esencial, ya que estos puntos proporcionan las coordenadas de referencia precisas necesarias para el algoritmo de calibraciÃ³n de la cÃ¡mara.

<table>
  <caption>
    <strong>Figura 1:</strong> Tableros de ajedrez con las esquinas detectadas.
  </caption>
  <tr>
    <td style="text-align: center;">
      <img src="..\Taller 1\Puntos_detectados\detected_imagen5.jpeg" width="90%" alt="Imagen 1">
      <p><strong>Figura 1.a</strong></p>
    </td>
    <td style="text-align: center;">
      <img src="../Taller 1\Puntos_detectados\detected_imagen3.jpeg" width="90%" alt="Imagen 2">
      <p><strong>Figura 1.b</strong> </p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="../Taller 1\Puntos_detectados\detected_imagen7.jpeg" width="90%" alt="Imagen 3">
      <p><strong>Figura 1.c</strong></p>
    </td>
    <td style="text-align: center;">
      <img src="../Taller 1\Puntos_detectados\detected_imagen6.jpeg" width="90%" alt="Imagen 4">
      <p><strong>Figura 1.d</strong> </p>
    </td>
  </tr>
</table>

El objetivo principal de la calibraciÃ³n es obtener la estimaciÃ³n de los parÃ¡metros que transforman una coordenada del mundo real (3D) en una coordenada de pÃ­xel en la imagen (2D). Para ello, se determinaron la matriz intrÃ­nseca de la cÃ¡mara y los coeficientes de distorsiÃ³n.

**EcuaciÃ³n 1:** Matriz IntrÃ­nseca de la CÃ¡mara ($\mathbf{K}$)

$$
\mathbf{K} = \begin{bmatrix}
1.344 \times 10^{3} & 0.0 & 801.86 \\
0.0 & 1.351 \times 10^{3} & 613.08 \\
0.0 & 0.0 & 1.0
\end{bmatrix}
$$

De la EcuaciÃ³n 1 se obtine la  observar la Matriz IntrÃ­nseca de la cÃ¡mara que esta compuesta por las longitudes focales $f_x$ y $f_y$, y las cordenadas $c_x$ y $c_y$ del centro Ã³ptico.  
Se observar que $f_x \approx 1344.23 \text{ pÃ­xeles}$ y $f_y \approx 1350.77 \text{ pÃ­xeles}$, la diferencia  aproximadamente 6.5 pÃ­xeles entre los ejes focales representa menos del $1\%$ del valor total, lo que confirma que la camara utiliza pÃ­xeles cuadrados, si esta diferencia resulta ser significativamente mayor ya estariamos hablando de pÃ­xeles rectangulares o que hay un problema, ya sea de aliasing (pÃ­xeles de un sensor es demasiado grande en comparaciÃ³n con los detalles finos de la escena) o escalado en la lectura del sensor. Respecto al centro Ã³ptico, los puntos $c_x = 801.86 \text{ pÃ­xeles}$ y $c_y = 613.08 \text{ pÃ­xeles}$ se encuentran muy prÃ³ximos al centro geomÃ©trico teÃ³rico de la imagen $(800.0, 600.0)$. El desplazamiento es mÃ­nimo, siendo de $1.86 \text{ pÃ­xeles}$ en $x$ y $13.08 \text{ pÃ­xeles}$ en $y$.

$$\text{Error RMS Total} = \frac{1}{N} \sum_{i=1}^{N} \left( \frac{1}{M_{i}} \sum_{j=1}^{M_{i}} \left\| \mathbf{m}_{i, j} - \mathbf{\hat{m}}_{i, j} \right\| \right)$$

Caption 1. FÃ³rmula utilizada para calcular el Error RMS Total de ReproyecciÃ³n. $N$ es el nÃºmero total de imÃ¡genes; $M_i$ es el nÃºmero de puntos detectados en la imagen $i$; $\mathbf{m}_{i, j}$ es la coordenada 2D real (observada) del punto; y $\mathbf{\hat{m}}_{i, j}$ es la coordenada 2D predicha (reproyectada) por el modelo de la cÃ¡mara.

El RMS obtenido fue de $0.495 \text{ pÃ­xeles}$, que es menor al umbral de $0.5 \text{ pÃ­xeles}$. Esto quiere decir que la ubicaciÃ³n real de las esquinas en la imagen y la ubicaciÃ³n predicha por el modelo matemÃ¡tico de la cÃ¡mara difieren en menos de medio pÃ­xel.

**EcuaciÃ³n 2:** Vector de Coeficientes de DistorsiÃ³n ($\mathbf{D}$)

$$
\mathbf{D} = \begin{pmatrix}
2.771 \times 10^{-1} & -1.895 & -7.036 \times 10^{-5} & 5.678 \times 10^{-4} & 4.138
\end{pmatrix}
$$

En la EcuaciÃ³n 2 se obtienen los coheficinetes de distorciÃ³n $k_1 = 0.277$ de primer orden, $k_2 = -1.895 $, $p_1 = -7.036 \times 10^{-5 } $ de segundo orden y $k_3 = 4.138$ de tercer orden. Cuando $k_1$ es negativo predomina la distorcion barril (las lineas se curvan hacia afuera), pero cuando $k_1$ es positivo se espera tener una distorsion cojin (las lineas se curvan hacia adentro). Como el coeficiente $k_1 > 0$ se espera que las imagenes muestren una distorsion cojin. 

Respecto a los coeficientes de distorsiÃ³n, se observa lo siguiente:

- Coeficiente $k_2$: Al ser un valor negativo, $k_2$ estÃ¡ actuando como un compensador o corrector para mitigar el efecto de una potencial subcorrecciÃ³n (o sobrecorrecciÃ³n) generada por $k_1$.

- Coeficiente $k_3$: El valor de $k_3$ es significativamente grande. Esto indica que la distorsiÃ³n de la lente es compleja o fuerte, lo que sugiere que el modelo de calibraciÃ³n no logrÃ³ un ajuste preciso de la distorsiÃ³n utilizando Ãºnicamente los coeficientes $k_1$ y $k_2$.

Los parÃ¡metros $p_1$ y $p_2$ miden la distorsiÃ³n tangencial, la cual provoca que un punto en el mundo real se proyecte en la imagen con un pequeÃ±o desplazamiento asimÃ©trico a lo largo de un vector que no es puramente radial.

Los valores obtenidos son: $p_1 = -7.036 \times 10^{-5}$ y $p_2 = 5.678 \times 10^{-4}$.

Al ser valores tan pequeÃ±os, esto demuestra una alineaciÃ³n fÃ­sica casi perfecta entre el eje Ã³ptico de la lente y el sensor de la cÃ¡mara, un factor ideal para la visiÃ³n artificial de precisiÃ³n.

<h4>Figura 2: ComparaciÃ³n de ImÃ¡genes (Original vs. Corregida)</h4>
<div style="display: grid; grid-template-columns: 50% 50%; gap: 10px;">
    
<!-- PAR 1 -->
<div style="text-align: center;">
    <img src="../Taller 1/Imagenes_calibracion/imagen2.jpeg" width="100%" alt="Imagen Original 1">
    <p><strong>(a.1) Original</strong></p>
</div>
<div style="text-align: center;">
    <img src="../Taller 1/Imagenes_corregidas/undistorted_imagen2.jpeg" width="100%" alt="Imagen Corregida 1">
    <p><strong>(b.1) Corregida</strong></p>
</div>

<!-- PAR 2 -->
<div style="text-align: center;">
    <img src="../Taller 1/Imagenes_calibracion/imagen3.jpeg" width="100%" alt="Imagen Original 2">
    <p><strong>(a.2) Original</strong></p>
</div>
<div style="text-align: center;">
    <img src="../Taller 1/Imagenes_corregidas/undistorted_imagen3.jpeg" width="100%" alt="Imagen Corregida 2">
    <p><strong>(b.2) Corregida</strong></p>
</div>

<!-- PAR 3 -->
<div style="text-align: center;">
    <img src="../Taller 1/Imagenes_calibracion/imagen7.jpeg" width="100%" alt="Imagen Original 3">
    <p><strong>(a.3) Original</strong></p>
</div>
<div style="text-align: center;">
    <img src="../Taller 1/Imagenes_corregidas/undistorted_imagen7.jpeg" width="100%" alt="Imagen Corregida 3">
    <p><strong>(b.3) Corregida</strong></p>
</div>
</div>

La correcciÃ³n de la distorsiÃ³n radial se evaluÃ³ con especial atenciÃ³n en las esquinas, ya que esta distorsiÃ³n aumenta proporcionalmente con la distancia al centro Ã³ptico $(ğ‘Ÿ)$, lo que hace que dicha zona requiera el mayor desplazamiento de pÃ­xeles. Visualmente, la Figura 2 confirma el Ã©xito, mostrando que las lÃ­neas del tablero, que en la imagen original se curvaban hacia el interior (distorsiÃ³n de tipo cojÃ­n), ahora son segmentos perfectamente rectos y paralelos, incluso en los bordes extremos. Esta correcciÃ³n robusta estÃ¡ respaldada por el bajo error RMS de reproyecciÃ³n $(0.495Â px)$, lo que indica que la predicciÃ³n de la posiciÃ³n de cada punto, incluidos los de las esquinas, se logrÃ³ con una precisiÃ³n subpÃ­xel.



## 3. Implementar y aplicar transformaciones de rotaciÃ³n y traslaciÃ³n

# 3. Implementar y aplicar transformaciones de rotaciÃ³n y traslaciÃ³n.

Creen una funciÃ³n que:

* **Cargue una imagen**
* **Aplique 5-8 transformaciones sucesivas** (traslaciones, rotaciones, escalas)
* **Genere un GIF animado o video** mostrando la secuencia
* **Guarden cada frame intermedio**


---
### Contexto

En el **procesamiento digital de imÃ¡genes**, las **transformaciones geomÃ©tricas** son operaciones que permiten modificar la posiciÃ³n, orientaciÃ³n o tamaÃ±o de los objetos dentro de una imagen. Entre las mÃ¡s comunes se encuentran la **traslaciÃ³n**, la **rotaciÃ³n** y la **escala**.

Estas operaciones son fundamentales en Ã¡reas como **visiÃ³n por computador**, **grÃ¡ficos por computadora** y **robÃ³tica**, donde es necesario manipular imÃ¡genes para anÃ¡lisis, reconstrucciÃ³n o animaciÃ³n.

El objetivo de este ejercicio es implementar un programa en **Python** que cargue una imagen, aplique una serie de transformaciones geomÃ©tricas sucesivas y genere un **GIF animado** que muestre la secuencia de transformaciones aplicadas.

---

### Fundamento TeÃ³rico


Las transformaciones geomÃ©tricas son operaciones fundamentales en el **procesamiento digital de imÃ¡genes**, ya que permiten modificar la posiciÃ³n, orientaciÃ³n o tamaÃ±o de los objetos dentro de una escena.

### a. TraslaciÃ³n:

Desplaza una imagen una distancia $(t_x, t_y)$ sobre los ejes $X$ y $Y$:

$$x' = x + t_x, \quad y' = y + t_y$$

### b. RotaciÃ³n:

Gira la imagen un Ã¡ngulo $\theta$ respecto a un punto de referencia:

$$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}$$

### c. Escalamento:

Aumenta o reduce el tamaÃ±o de una imagen segÃºn un factor $s$:

$$x' = s_x \cdot x, \quad y' = s_y \cdot y$$

Combinadas, estas operaciones permiten realizar **transformaciones afines** que preservan las lÃ­neas y proporciones de la imagen original.



### MetodologÃ­a

1. **Carga de la imagen original:**  
   Se utilizÃ³ la librerÃ­a `PIL` para abrir y convertir la imagen a formato RGBA, garantizando compatibilidad con transparencia.

2. **DefiniciÃ³n de transformaciones:**  
   Se definiÃ³ una lista de diccionarios con los parÃ¡metros de **rotaciÃ³n (rot)**, **traslaciÃ³n (tx, ty)** y **escala (scale)** para cada paso sucesivo.

3. **AplicaciÃ³n secuencial:**  
   Para cada transformaciÃ³n:
   - Se reescalÃ³ la imagen.
   - Se rotÃ³ segÃºn el Ã¡ngulo indicado.
   - Se pegÃ³ sobre un fondo blanco, ajustando la posiciÃ³n con traslaciÃ³n.

4. **GeneraciÃ³n de frames:**  
   Cada imagen transformada se guardÃ³ como `frame_XX.png` en la carpeta `frames_temp/`.

5. **CreaciÃ³n del GIF:**  
   Finalmente, con `imageio.mimsave()` se combinan los cuadros en un GIF animado llamado **`transformaciones.gif`**.

---

### CÃ³digo Implementado

El siguiente script en Python aplica **transformaciones geomÃ©tricas** (rotaciÃ³n, traslaciÃ³n y escalamiento) a una imagen para generar un **GIF animado**.  
AdemÃ¡s, incluye una verificaciÃ³n para evitar volver a ejecutar la funciÃ³n si los resultados ya existen.

---

### ğŸ§© CÃ³digo fuente

```python
from PIL import Image
import imageio
import os
import numpy as np

def transformar_imagen(ruta_imagen, salida_gif="animacion.gif"):
    carpeta_frames = "frames_temp"

    # ğŸš« Si ya existen los resultados, no ejecutar la funciÃ³n
    if os.path.exists(carpeta_frames) and os.path.exists(salida_gif):
        print("âš ï¸ Las carpetas y el GIF ya existen. No se ejecutarÃ¡ la funciÃ³n.")
        return

    # Crear carpeta temporal para los frames (solo si no existe)
    os.makedirs(carpeta_frames, exist_ok=True)

    # Cargar imagen original
    img_original = Image.open(ruta_imagen).convert("RGBA")
    ancho, alto = img_original.size

    # Lista para guardar los frames
    frames = []

    # Definir transformaciones (rotaciÃ³n, traslaciÃ³n, escala)
    transformaciones = [
        {"rot": 0,   "tx": 0,  "ty": 0,  "scale": 1.0},
        {"rot": 15,  "tx": 20, "ty": 0,  "scale": 1.1},
        {"rot": 30,  "tx": 40, "ty": 10, "scale": 1.2},
        {"rot": 60,  "tx": 60, "ty": 20, "scale": 1.1},
        {"rot": 90,  "tx": 80, "ty": 40, "scale": 0.9},
        {"rot": 120, "tx": 60, "ty": 60, "scale": 0.8},
        {"rot": 150, "tx": 40, "ty": 80, "scale": 0.9},
        {"rot": 180, "tx": 0,  "ty": 100,"scale": 1.0},
    ]

    for i, t in enumerate(transformaciones):
        # Aplicar escala
        nuevo_ancho = int(ancho * t["scale"])
        nuevo_alto = int(alto * t["scale"])
        img_transformada = img_original.resize((nuevo_ancho, nuevo_alto))

        # Aplicar rotaciÃ³n
        img_transformada = img_transformada.rotate(t["rot"], expand=True)

        # Crear fondo blanco
        fondo = Image.new("RGBA", (ancho * 2, alto * 2), (255, 255, 255, 255))

        # Calcular posiciÃ³n
        x = ancho // 2 + t["tx"]
        y = alto // 2 + t["ty"]

        # Pegar imagen transformada
        fondo.paste(img_transformada, (x, y), img_transformada)

        # Guardar frame
        frame_path = os.path.join(carpeta_frames, f"frame_{i:02d}.png")
        fondo.save(frame_path)
        frames.append(imageio.v3.imread(frame_path))

    # Crear GIF con loop infinito
    imageio.mimsave(salida_gif, frames, duration=0.5, loop=0)

    print(f"âœ… GIF generado correctamente: {salida_gif}")
    print(f"ğŸ“‚ Frames guardados en: {os.path.abspath(carpeta_frames)}")

# Ejemplo de uso
transformar_imagen("../images/Imagen_gif.jpeg", "transformaciones.gif")

```

---

### **Resultados**

- Se generÃ³ correctamente el archivo **`transformaciones.gif`**, mostrando la secuencia de transformaciones aplicadas sobre la imagen original.  
- Los frames intermedios fueron almacenados en la carpeta **`frames_temp`**, permitiendo su inspecciÃ³n individual.
- El movimiento obtenido refleja un cambio progresivo en posiciÃ³n, tamaÃ±o y orientaciÃ³n de la figura, generando una animaciÃ³n fluida y continua.

**Archivos generados:**
```
frames_temp/frame_00.png
frames_temp/frame_01.png
...
frames_temp/frame_07.png
transformaciones.gif
```

---

### **AnÃ¡lisis**

El resultado demuestra la correcta aplicaciÃ³n de transformaciones afines en el plano bidimensional.  
Al combinar **rotaciÃ³n**, **traslaciÃ³n** y **escala**, se observa un efecto dinÃ¡mico que simula movimiento.  
AdemÃ¡s, la funciÃ³n implementa un mecanismo de **control de ejecuciÃ³n** que evita repetir cÃ¡lculos si los archivos ya existen, optimizando el flujo de trabajo.

**Aspectos destacados:**
- El uso de `expand=True` en la rotaciÃ³n evita recortes de la imagen.  
- La creaciÃ³n de un lienzo de fondo mayor garantiza que la imagen transformada siempre se mantenga visible.  
- El GIF permite visualizar la evoluciÃ³n progresiva de la imagen, siendo Ãºtil para comprender los efectos de cada transformaciÃ³n.


## 5. Implementar tÃ©cnicas fundamentales de segmentaciÃ³n de imÃ¡genes.
Capturen una escena con objetos de colores distintos de la Universidad Nacional o de la oficina de alguno de los integrantes del equipo. Debe ser con una cÃ¡mara de un telÃ©fono celular.
- Segmenten cada objeto por su color
- Cuenten cuÃ¡ntos objetos de cada color hay
- Calculen el Ã¡rea de cada objeto


### SoluciÃ³n

Para analizar una fotografÃ­a de orquÃ­deas capturada en la Universidad Nacional con un telÃ©fono Motorola G30, se diseÃ±Ã³ un pipeline de visiÃ³n por computadora con el fin de superar el principal desafÃ­o de la escena: el solapamiento y la oclusiÃ³n entre las flores.

**1. SegmentaciÃ³n de Objetos por Color**

El proceso inicia con la segmentaciÃ³n por color, convirtiendo la imagen al espacio **HSV** para robustez ante variaciones de luz. Se definieron categorÃ­as de color ('Rosa', 'Azul Pastel', 'Amarillo Quemado') y una clase especial ('Blancas') que agrupa los tonos blanco y rosa pÃ¡lido mediante una operaciÃ³n lÃ³gica `cv2.bitwise_or`. Los rangos de color se establecieron con un mÃ©todo hÃ­brido: se tomaron valores base de la plataforma HTML Color Codes (2025) y se ajustaron empÃ­ricamente analizando la fotografÃ­a original.

**2. Conteo de Objetos**

Una vez aislados los objetos, se utilizÃ³ el algoritmo `cv2.findContours` para detectar cada elemento individual. Sin embargo, para garantizar un conteo preciso, se implementÃ³ una lÃ³gica de refinamiento que:
* **Fusiona contornos:** Une fragmentos que pertenecen a un solo objeto pero que fueron detectados por separado.
* **Divide contornos:** Separa objetos que se tocan y fueron errÃ³neamente detectados como uno solo.

Este proceso de post-procesamiento asegura que el nÃºmero final de objetos contados por color sea una representaciÃ³n fiel de la escena real.

**3. CÃ¡lculo del Ãrea de Cada Objeto**

Finalmente para que las mediciones tuvieran validez en el mundo real, se implementÃ³ un sistema de calibraciÃ³n de escala. Se tomÃ³ como referencia el diÃ¡metro promedio de una flor de orquÃ­dea, establecido en 10 cm segÃºn la literatura consultada (JardÃ­n BotÃ¡nico de MedellÃ­n, 2024). El script mide este diÃ¡metro de referencia en pÃ­xeles y calcula un factor de conversiÃ³n **(pÃ­xeles por centÃ­metro)**. Finalmente, el Ã¡rea de cada objeto detectado, inicialmente calculada en pÃ­xeles con `cv2.contourArea()`, se convierte a **centÃ­metros cuadrados (cmÂ²)** utilizando esta escala.

![Imagen de las flores utilizadas en el anÃ¡lisis y resultados](images/DashBoardFlores.png)

### AnÃ¡lisis de Resultados
***

En esta secciÃ³n se evalÃºa el rendimiento del pipeline de visiÃ³n por computadora implementado, comparando los resultados cuantitativos obtenidos por el script con una inspecciÃ³n manual de la imagen (Ground Truth). El objetivo es identificar tanto los aciertos como las limitaciones del modelo y proponer mejoras tÃ©cnicas.

#### Comparativa de Resultados: PredicciÃ³n vs. Realidad

| CategorÃ­a de Color | Cantidad Real (Ground Truth) | Cantidad Predicha (Modelo) | PrecisiÃ³n |
| :----------------- | :--------------------------: | :------------------------: | :-------: |
| Rosa Â  Â  Â  Â  Â  Â  Â  | Â  Â  Â  Â  Â  Â  Â 7 Â  Â  Â  Â  Â  Â  Â  | Â  Â  Â  Â  Â  Â  7 Â  Â  Â  Â  Â  Â  Â | Â 100% Â  Â  |
| Amarillo Quemado Â  | Â  Â  Â  Â  Â  Â  Â 7 Â  Â  Â  Â  Â  Â  Â  | Â  Â  Â  Â  Â  Â  8 Â  Â  Â  Â  Â  Â  Â | Â 87.5% Â  Â |
| Azul Pastel Â  Â  Â  Â | Â  Â  Â  Â  Â  Â  Â 1 Â  Â  Â  Â  Â  Â  Â  | Â  Â  Â  Â  Â  Â  1 Â  Â  Â  Â  Â  Â  Â | Â 100% Â  Â  |
| Blanco Â  Â  Â  Â  Â  Â  | Â  Â  Â  Â  Â  Â  Â 13 Â  Â  Â  Â  Â  Â  Â | Â  Â  Â  Â  Â  Â  12 Â  Â  Â  Â  Â  Â  | Â 92.3% Â  Â |
| **Total** | Â  Â  Â  Â  Â  Â **28** | Â  Â  Â  Â  Â  **28** | Â **96.4%**|

*Nota: La precisiÃ³n se calcula como (Predichos Correctos / Total Real). Para el caso del Amarillo, 7 de 8 detecciones correspondÃ­an a flores reales.*

***
#### DiscusiÃ³n de Aciertos y DesafÃ­os

El modelo demostrÃ³ un alto rendimiento general, con una precisiÃ³n global del 96.4% en la identificaciÃ³n y conteo de objetos.

**Aciertos Notables:**

* Las categorÃ­as **Rosa** y **Azul Pastel** fueron identificadas con un 100% de precisiÃ³n. Esto valida que los rangos HSV definidos y la lÃ³gica de segmentaciÃ³n son altamente efectivos para objetos con colores bien definidos y formas consistentes.

**DesafÃ­os y Puntos de Mejora:**

* **Flores Amarillas (Sobre-segmentaciÃ³n):** El modelo predijo 8 flores cuando en realidad habÃ­a 7. El anÃ¡lisis cualitativo revela que una Ãºnica flor fue segmentada incorrectamente en dos partes, resultando en un conteo adicional. Este error de **sobre-segmentaciÃ³n** probablemente se deba a sombras pronunciadas o gradientes de color internos en la flor, que la lÃ³gica de divisiÃ³n interpretÃ³ errÃ³neamente como el lÃ­mite entre dos objetos.

* **Flores Blancas (Sub-segmentaciÃ³n y DetecciÃ³n de Bordes):** Se predijeron 12 flores de las 13 existentes. El error se originÃ³ en la dificultad del modelo para definir correctamente los bordes de dos flores que presentaban una transiciÃ³n de color compleja (de blanco marfil oscuro a rosa pÃ¡lido). Esto causÃ³ que una de las flores no fuera detectada en su totalidad, llevando a un error de **sub-segmentaciÃ³n**. La categorÃ­a 'Blancas', al combinar dos rangos de color, es inherentemente mÃ¡s compleja y sensible a estas variaciones sutiles.

***
#### Mejoras Futuras Propuestas

1. Â **Refinar la LÃ³gica de FusiÃ³n/DivisiÃ³n:** La divisiÃ³n basada en el rectÃ¡ngulo delimitador es efectiva pero simple. Para evitar la sobre-segmentaciÃ³n (caso amarillo), se podrÃ­a implementar el **Algoritmo de Watershed**. Esta tÃ©cnica es mÃ¡s robusta para separar objetos que se tocan, ya que se basa en la "topografÃ­a" de la imagen en lugar de su geometrÃ­a simple.

2. Â **Mejorar la SegmentaciÃ³n de Colores Complejos:** Para el caso de las flores blancas, en lugar de combinar dos rangas con un `OR` lÃ³gico, se podrÃ­a explorar el uso de tÃ©cnicas de **clustering de color (como K-Means)** en regiones de interÃ©s. Esto permitirÃ­a agrupar pÃ­xeles de manera mÃ¡s inteligente, adaptÃ¡ndose mejor a las transiciones de tono.

3. Â **Pre-procesamiento Adicional:** Aplicar un filtro de suavizado, como un **desenfoque gaussiano (Gaussian Blur)** de bajo nivel antes de la segmentaciÃ³n, podrÃ­a ayudar a homogeneizar las superficies de las flores. Esto reducirÃ­a el impacto de texturas y sombras internas, minimizando el riesgo de errores como el ocurrido con la flor amarilla.



## Referencias
GeeksforGeeks. (s.f.). *Camera Calibration with Python OpenCV*. https://www.geeksforgeeks.org/python/camera-calibration-with-python-opencv/

Reolink. (s.f.). Barrel Distortion: What It Is, and How to Fix It. Reolink Blog. https://reolink.com/blog/barrel-distortion/

OpenCV Development Team. (s.f.). Camera Calibration and 3D Reconstruction. Obtenido de la documentaciÃ³n oficial de OpenCV, mÃ³dulo calib3d.

Szeliski, R. (2021). Computer Vision: Algorithms and Applications (2.Âª ed.). Springer.

HTML Color Codes. (2025). HTML Color Codes. Recuperado el 19 de octubre de 2025, de https://htmlcolorcodes.com/es/

JardÃ­n BotÃ¡nico de MedellÃ­n. (2024). *Ficha tÃ©cnica: Phalaenopsis amabilis*. Recuperado de https://www.jardinbotanicomedellin.org/orquideas/phalaenopsis


Gonzalez, R. C., & Woods, R. E. (2018). *Digital Image Processing (4th Edition)*. Pearson.  
Pillow Documentation: [https://pillow.readthedocs.io](https://pillow.readthedocs.io)  
ImageIO Documentation: [https://imageio.readthedocs.io](https://imageio.readthedocs.io)